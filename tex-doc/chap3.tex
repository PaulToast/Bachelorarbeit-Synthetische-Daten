% !TEX root = Bachelorarbeit Synthetische Daten.tex
\chapter{Methodisches Vorgehen}

In diesem Kapitel wird das methodische Vorgehen der Arbeit beschrieben. Als Basis für die Untersuchung der Forschungsfragen wird zunächst der MVIP-Datensatz vorgestellt, auf dem die Experimente durchgeführt werden. Anschließend wird die Implementierung der Modelle DA-Fusion und Supervised Contrastive Learning erläutert, die für die synthetische Datengenerierung und die Klassifikationsaufgabe verwendet werden. Es wird auch auf die Anpassungen eingegangen, die vorgenommen werden mussten, um den eigenen Ansatz zu testen. Danach wird die Herangehensweise zur Generierung synthetischer Daten mit DA-Fusion und die Trainings- und Testdurchläufe mit Supervised Contrastive Learning definiert. Abschließend werden die Evaluationsmethoden und Metriken vorgestellt, die zur Auswertung der Experimente verwendet werden.

\section{MVIP-Datensatz} \label{sec:dataset}

% EIBA-Artikel: \parencite{ReziProK2019eiba}
% MVIP-Datensatz: \parencite{Koch2023mvip}

Grundlage der Forschungsarbeit ist der im Rahmen des EIBA-Projekts entstandene MVIP-Datensatz \parencite{Koch2023mvip}, wobei MVIP für \textit{Multi-View Industrial Parts} steht. Die Bilddaten stammen aus Intel RealSense D435 und D415 Tiefenkameras, die die Objekte gleichzeitig aus verschiedenen Perspektiven aufnehmen. Darüber hinaus gibt es auch Metadaten, etwa zum Gewicht des Objekts, oder Beschreibungen in natürlicher Sprache durch verschiedene Stichwörter ("StarterMotor", "used", "rusty", usw.).

% Details: Klassen und Anzahl der Bilder insgesamt, Verteilung der Klassen
Der Datensatz enthält 308 Klassen , welche wiederum in 18 verschiedene Oberklassen (Super Classes) eingeteilt sind. Insgesamt gibt es etwa 71.276 Sets an Bildern, welche jeweils RGB- und Tiefendaten, sowie Segmentierungsmasken enthalten.

% Beispielbilder, -masken, -metadaten
\begin{figure}
	\centering
	%\includegraphics[width=\textwidth]{figure_mvip_ex_cropped_1.png}
	\includegraphics[width=\textwidth]{figure_mvip_ex_cropped_2.png}
	\caption{Beispielbilder aus dem MVIP-Datensatz \parencite{Koch2023mvip},\\
	die auf die Region of Interest (ROI) zugeschnitten wurden.}
	\label{fig:mvip-examples}
\end{figure}

\subsection{Teildatensatz} \label{sec:subdataset}

% Wahl eines Teildatensatzes mit 20 "CarComponent" Klassen
Um die Rechenzeit zu reduzieren und die Experimente auf eine bestimmte Objektkategorie zu beschränken, wurde ein Teildatensatz des EIBA-Datensatzes verwendet.

Konkret wurden zufällig 20 Klassen aus der Oberklasse "CarComponent" ausgewählt. Für die Experimente wurden außerdem ausschließlich die RGB-Bilder verwendet, wobei die Objektmasken im Pre-Processing der Daten zum Einsatz kommen. Insgesamt enthält der Teildatensatz ... Bilder.

% Beispielbilder, -masken ???
...

\subsection{Vorverarbeitung} \label{sec:preprocessing}

% Weil Masking nicht direkt implementiert ist: Verwendung der Objektmasken, um die Bilder zu croppen
	% Bounding Box
	% Quadratischer Output
	% Weniger Aufmerksamkeit auf gleichbleibenden Hintergrund
% Verschiedene "klassische" Augmentationen
	% Rotation
	% ColorJitter
% Normalisierung der Bilder
...

\section{Implementierung} \label{sec:implementation}

% Arbeitsumgebung, Rechner-Zugang, etc.
% Verwendung vorhandener Implementierungen
% Gemeinsame Programmiersprache, Bibliotheken, etc.

Für die Vorbereitung der in dieser Arbeit durchgeführten Experimente konnte sich größtenteils auf die Implementierung von DA-Fusion und Supervised Contrastive Learning aus den Quellen ... und ... gestützt werden. Beide Implementierungen sind in Python geschrieben und verwenden die Bibliothek PyTorch. ...

Dennoch mussten einige Anpassungen vorgenommen werden, um die Modelle auf den MVIP-Teildatensatz anzuwenden, und um die synthetischen Augmentationen aus DA-Fusion im Supervised Contrastive Learning zu verwenden. ...

\subsection{DA-Fusion} \label{sec:impl-da-fusion}

% Bestehende Implementierung verstehen
% Klasse zum Laden des MVIP-Teildatensatzes
	% Masken-cropping
	% Masken für Augmentation
% Workflow-Anpassungen für Input/Output

In ...'s Implementierung von DA-Fusion wird zunächst mit Textual Inversion ein vortrainiertes Stable Diffusion-Modell fine-tuned, indem ein neuer Token für jede Klasse im Datensatz erlernt wird. Um anschließend die Augmentationen zu generieren, werden die Bilder des Datensatzes genommen, Rauschen hinzugefügt und unter Konditionierung auf den entsprechenden Token wiederhergestellt. Je nachdem, wie viel Rauschen hinzugefügt wurde, entstehen so mehr oder weniger stark veränderte Bilder, die als synthetische Daten verwendet werden können.

...

Die Implementierung von DA-Fusion kann weitgehend unverändert angewendet werden, um synthetische Daten für den MVIP-Teildatensatz zu generieren. Es muss lediglich eine eigene Klasse für den Datensatz erstellt werden, die die Bilder und Masken aus dem MVIP-Datensatz lädt und die Token für die Klassen bereitstellt. ...

\subsection{Supervised Contrastive Learning} \label{sec:impl-supcon}

% Bestehende Implementierung verstehen
% Klasse zum Laden des MVIP-Teildatensatzes und der Augmentationen
% Workflow-Anpassungen für Input/Output (wie DA-Fusion)
% Parameter für Konfiguration der Verwendung der Augmentationen
% Integration von OOD-Augmentationen im Contrastive Learning
% Metriken für Evaluation
	% Accuracy
	% ID- und OOD-Confidence

...'s Implementierung von Supervised Contrastive Learning beinhaltet drei Trainings-Skripte; eines für das Pre-Training der latenten Repräsentationen unter Verwendung der Supervised Contrastive Loss-Funktion, eines für die lineare Klassifikation der Repräsentationen und eines zum Training eines klassischen Klassifikator-Modells mit Cross Entropy Loss (zum Vergleich).

...

Auch hier muss eine eigene Klasse für den MVIP-Teildatensatz erstellt werden, die die Bilder und Masken lädt und die synthetischen Daten von DA-Fusion bereitstellt. Die Klasse muss nun auch Parameter bereitstellen, die die Verwendung der synthetischen Daten steuern, z.B. ob keine Augmentationen, ausschließlich positiv-Beispiele oder auch negativ-Beispiele verwendet werden sollen. ...

...

\section{Synthetische Datengenerierung mit DA-Fusion} \label{sec:synt-gen-da-fusion}

% ID-Augmentationen, OOD-Augmentationen
% Trial and Error, um Parameter zu bestimmen, Validierung ist schwierig
...

\section{Trainings- und Testdurchläufe mit Supervised Contrastive Learning} \label{sec:train-test-supcon}

% Training ist aufgeteilt in Pre-Training und Lineare Klassifikation
% Drei Versuche
	% Versuch 1: - Nur reale Daten; sowohl Pre-Training als auch Klassifikation
	% Versuch 2: - Mit ID-Augmentationen; sowohl Pre-Training als auch Klassifikation
	% Versuch 3: - Wie 2., aber Contrastive Pre-Training Near OOD-Augmentationen als Hard Negatives
...

\section{Evaluationsmethoden und Metriken} \label{sec:evaluation}

% Menschliche Evaluierung der Augmentationen selbst
% Accuracy, ID- und OOD-Confidence des SCL Klassifikators
...

% Pre-Training

% Lineare Klassifikation